{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neuron functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def exp(z):\n",
    "    return np.exp(z)\n",
    "\n",
    "def I(z): # identity function\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss/Cost functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAPE(pred, tgt):\n",
    "    N = tgt.shape[0]\n",
    "    I = tgt > 0\n",
    "    tgt = tgt[I]\n",
    "    pred = pred[I]\n",
    "    return np.sum(np.divide(np.abs(pred - tgt), tgt)) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**derivative functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d(a, f):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        a: a matrix of activations, a = f(z) for some input matrix z\n",
    "        f: a function such that f(z) = a\n",
    "    What it does:\n",
    "        determines a' = df(z)/dz, the derivative of a with respect to z.\n",
    "        For example, if a = exp(z) then a' = a.\n",
    "    Outputs:\n",
    "        the derivative of a with respect to z\n",
    "    \"\"\"\n",
    "    return (np.ones_like(a) if f == I else\n",
    "           np.sign(a) if f == relu else\n",
    "           a*(1 - a) if f == sigmoid else\n",
    "           a if f == exp else ValueError('bad function type'))\n",
    "\n",
    "def dMAPE(pred, tgt):\n",
    "    N = tgt.shape[0]\n",
    "    return np.divide( np.multiply(tgt, np.sign(pred - tgt)), N*(np.multiply(tgt, tgt) + np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomization functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Gauss(n, s):\n",
    "    return n*(1 + np.random.randn()/s)\n",
    "\n",
    "def asymptotic(n, s):\n",
    "    return 1 - (1 - n)/(1 + np.random.randn()/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, flavors, eta, alpha, batch_size, epochs, loss, max_inputs):\n",
    "        np.random.seed(1729)\n",
    "        self.sizes = sizes # number of nodes in each layer, including input layer\n",
    "        self.sizes[0] = min(self.sizes[0], max_inputs)\n",
    "        self.flavors = flavors # function executed by nodes in given layer, excluding input layer\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.max_inputs = max_inputs\n",
    "        self.weights = self.__get_weights__()\n",
    "        self.biases = self.__get_biases__()\n",
    "        \n",
    "    def __get_weights__(self):\n",
    "        return [np.matrix(0.01*np.random.rand(fro, to)) \n",
    "                for (to, fro) in zip(self.sizes[1:], self.sizes[:-1])]\n",
    "\n",
    "    def __get_biases__(self):\n",
    "        return [np.matrix(0.01*np.random.rand(to, 1)).T for to in self.sizes[1:]]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            X: a matrix containing rows of input vectors\n",
    "        What it does:\n",
    "            Ths is the feed forward algorithm, It iterates through the network object, \n",
    "            updating the input vector X at each layer of the network using the weights, \n",
    "            biases and node function\n",
    "        Outputs:\n",
    "            a list of matrices corresponding to the activations at each layer of\n",
    "            the matrix. For example, the first matrix is the input matrix, the last\n",
    "            matrix is the prediction matrix. Intermediate matrices are the\n",
    "            activations output by the hidden layers\n",
    "        \"\"\"\n",
    "        A = [X]\n",
    "        for i in xrange(len(self.flavors)):\n",
    "            W, B, F = self.weights[i], self.biases[i], self.flavors[i]\n",
    "            X = F(X*W + B)\n",
    "            A.append(X)\n",
    "        return A\n",
    "            \n",
    "    def __backprop__(self, A, Y):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            A: a list containing matrices of activations for each layer of the\n",
    "            network for the current minibatch\n",
    "            Y: a list of targets for the curent minibatch\n",
    "        What it does:\n",
    "            given a batch of data, finds the gradients of the loss function\n",
    "            with respect to each of the weight and bias matrices\n",
    "        Outputs:\n",
    "            a list containing all partials with respect to weight matrices\n",
    "            and a list containing all partials with respect to bias matrices\n",
    "        \"\"\"\n",
    "        nabla_CB, nabla_CW = [], []\n",
    "        delta = dMAPE(A[-1], Y)\n",
    "        seno = np.ones_like(Y).T # row ones vector\n",
    "        for i in xrange(1, len(A)):\n",
    "            # calculate z-derivative of current activation\n",
    "            da = d(A[-i], self.flavors[-i])\n",
    "            delta_da = np.multiply(delta, da)\n",
    "            nabla_CB.insert(0, seno*delta_da)\n",
    "            nabla_CW.insert(0, A[-i-1].T*(delta_da))\n",
    "            delta = (delta_da)*self.weights[-i].T\n",
    "        return nabla_CB, nabla_CW\n",
    "    \n",
    "    def train(self, X_fit, Y_fit):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            X_fit: the portion of the training feature matrix used for fitting\n",
    "            Y_fit:  the portion of the training target matrix used for fitting\n",
    "            X_val: the portion of the training feature matrix used for validation\n",
    "        What it does:\n",
    "            Iterates over epochs in which the entire fit data matrix is used to\n",
    "            train the network. Controls the batch generation and selection within\n",
    "            epochs. Feeds batches one at a time into the feedforward and\n",
    "            backpropagation functions. Updates the network object's weights and\n",
    "            biases with the negative gradients derived by back prop and scaled by\n",
    "            the learning rate factor. Decays the learning rate each epoch.\n",
    "        Outputs:\n",
    "            an array of predictions for the validation data from the fully trained\n",
    "            network\n",
    "        \"\"\"\n",
    "        np.random.seed(1729)\n",
    "        eta = self.eta # make a copy as we're gonna decay it over time\n",
    "        for ep in xrange(self.epochs):\n",
    "            n = X_fit.shape[0]\n",
    "            indices = np.random.shuffle([i for i in xrange(n)])\n",
    "            batches = n / self.batch_size + 1 # cld --> ceiling division\n",
    "            batch_size = 1.*n / batches # the used batch size is not precisely what\n",
    "                                     # was requested, but optimal given the\n",
    "                                     # amount of data\n",
    "            k = 0\n",
    "            while k < batches:\n",
    "                lo = int(k*batch_size)\n",
    "                hi = int((k+1)*batch_size)\n",
    "                A_list = self.predict(X_fit[lo:hi,:])\n",
    "                nabla_CB, nabla_CW = self.__backprop__(A_list, Y_fit[lo:hi])\n",
    "                self.weights = ([w - eta*dCw for (w, dCw) in zip(self.weights, nabla_CW)])\n",
    "                self.biases = ([b - eta*dCb for (b, dCb) in zip(self.biases, nabla_CB)])\n",
    "                k += 1\n",
    "            eta *= self.alpha\n",
    "\n",
    "    def HyparamGen(self):\n",
    "        np.random.seed()\n",
    "        self.sizes = [int(Gauss(sz, 10)) for sz in self.sizes[0:-1]] + [1] # 1 on end\n",
    "        self.sizes[0] = min(self.sizes[0], self.max_inputs)\n",
    "        self.eta = Gauss(self.eta, 10)\n",
    "        self.alpha = asymptotic(self.alpha, 10)\n",
    "        self.epochs = int(Gauss(self.epochs, 10))\n",
    "        np.random.seed(1729)\n",
    "        self.weights = self.__get_weights__()\n",
    "        self.biases = self.__get_biases__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_features(X, Y):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        X: the full feature matrix\n",
    "        Y: the full target matrix\n",
    "    What it does:\n",
    "        Finds the top features (columns) in the training feature matrix according to \n",
    "        correlation to the training target matrix.\n",
    "    Outputs:\n",
    "        the indices of the feature columns in order of correlation importance\n",
    "    \"\"\"\n",
    "    lyste = abs(X.T * Y)\n",
    "    indices = []\n",
    "    for _ in xrange(len(lyste)):\n",
    "        i = np.argmax(lyste)\n",
    "        lyste[i] = 0.\n",
    "        indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CrossValidation(net, X_train, Y_train, X_test, Y_test, CV):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        net: a neural network object\n",
    "        X_train: the training feature matrix\n",
    "        Y_train: the training target matrix\n",
    "        X_test: the test feature matrix\n",
    "        Y_test: the test target matrix\n",
    "        CV: an array of arrays containg the fit and train indices for each\n",
    "        cross-validation fold\n",
    "    What it does:\n",
    "        Iterates over cross-validation folds, splitting the data into\n",
    "        appropriate fit and validation sets, and passing the fit and val sets\n",
    "        on to the train function. Assembles validation predictions from the\n",
    "        train function into a comprehensive list over the entire training data\n",
    "        set. Assembles test data predictions from the feedforward function for\n",
    "        every fold.\n",
    "    Outputs:\n",
    "        The validation loss, using whatever loss functon is defined for the\n",
    "        network\n",
    "        The mean of the test data predictions from each fold of the cross\n",
    "        validation\n",
    "    \"\"\"\n",
    "    psi = np.matrix(np.empty((0, 1)))\n",
    "    Y = np.matrix(np.empty((0, 1)))\n",
    "    psi_test = np.matrix(np.empty((X_test.shape[0], 0)))\n",
    "    for in_fit, in_val in CV:\n",
    "        X_fit, Y_fit = X_train[in_fit,:], Y_train[in_fit]\n",
    "        X_val, Y_val = X_train[in_val,:], Y_train[in_val]\n",
    "        #training\n",
    "        net.train(X_fit, Y_fit)\n",
    "        psi = np.vstack((psi, net.predict(X_val)[-1]))\n",
    "        Y = np.vstack((Y, Y_val))\n",
    "        psi_test = np.hstack((psi_test, net.predict(X_test)[-1]))\n",
    "    return (net.loss(psi, Y), net.loss(np.mean(psi_test, axis=1), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "import copy\n",
    "\n",
    "def PerpetualSearch(net, X, Y, aktuellMAPE):\n",
    "    folds = 3\n",
    "    correlindices = top_features(X, Y)\n",
    "    np.random.seed(1729)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    optimal_net = copy.deepcopy(net)\n",
    "    while True:\n",
    "        print \"doing something\"\n",
    "        # focus on top N features\n",
    "        tops = correlindices[0:net.sizes[0]]\n",
    "        X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "        currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                       X_test_tops, Y_test, CVfolds)\n",
    "        with open(\"Record.csv\",\"a+\") as f:\n",
    "            for i in range(6):\n",
    "                if i < len(net.sizes)-1:\n",
    "                    f.write(\"%s\\t\" % net.sizes[i])\n",
    "            f.write(\"%s\\t\" % net.eta)\n",
    "            f.write(\"%s\\t\" % net.alpha)\n",
    "            f.write(\"%s\\t\" %net.epochs)\n",
    "            f.write(\"%s\\t\" % currentMAPE)\n",
    "            f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "            if currentMAPE < aktuellMAPE:\n",
    "                net.train(X_train_tops, Y_train)\n",
    "                true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                f.write(\"%s\\n\" % true_test_loss)  \n",
    "                optimal_net = copy.deepcopy(net)\n",
    "                aktuellMAPE = currentMAPE\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        net = copy.deepcopy(optimal_net)\n",
    "        net.HyparamGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Grid Search. \n",
    "\n",
    "Do this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('TrainingData.csv')\n",
    "\n",
    "Y = np.matrix(training_data.iloc[:,2].values).T\n",
    "X = np.matrix(training_data.iloc[:,:].values)\n",
    "\n",
    "training_data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = 3\n",
    "correlindices = top_features(X, Y)\n",
    "np.random.seed(1729)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "aktuellMAPE = np.infty\n",
    "\n",
    "for w in [[10,10,10,10], [20,20,20,20], [50,50,50,50], [100,100,100,100]]:\n",
    "    for eta in [0.1, 0.5, 1]:\n",
    "        for alpha in [0.999, 0.99, 0.9]:\n",
    "            for eps in [100, 200, 300, 400]:\n",
    "                net = Network(\n",
    "                              w + [1],          # nodes per layer\n",
    "                              [relu, relu, relu, relu],     # activation functions at each hidden and output layer\n",
    "                              eta,                                     # learning rate\n",
    "                              alpha,                                    # learning rate decay\n",
    "                              2048,                                    # desired batch size\n",
    "                              eps,                                     # number of epochs\n",
    "                              MAPE,                                    # loss function\n",
    "                              X.shape[1]                               # max number of features\n",
    "                             )\n",
    "                print \"doing something\"\n",
    "                # focus on top N features\n",
    "                tops = correlindices[0:net.sizes[0]]\n",
    "                X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "                currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                       X_test_tops, Y_test, CVfolds)\n",
    "                with open(\"Record.csv\",\"a+\") as f:\n",
    "                    for sz in net.sizes[:-1]:\n",
    "                        f.write(\"%s\\t\" % sz)\n",
    "                    f.write(\"%s\\t\" % net.eta)\n",
    "                    f.write(\"%s\\t\" % net.alpha)\n",
    "                    f.write(\"%s\\t\" %net.epochs)\n",
    "                    f.write(\"%s\\t\" % currentMAPE)\n",
    "                    f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "                    if currentMAPE < aktuellMAPE:\n",
    "                        net.train(X_train_tops, Y_train)\n",
    "                        true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                        f.write(\"%s\\n\" % true_test_loss)  \n",
    "                        optimal_net = copy.deepcopy(net)\n",
    "                        aktuellMAPE = currentMAPE\n",
    "                    else:\n",
    "                        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the best known hyperparameters using perpetual search after classic grid search has completed. \n",
    "\n",
    "Perpetual search will run indefinitely; you have to interrupt the kernel to sop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize network object as starting point for perpetual search\n",
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "aktuellMAPE = record.iloc[i, 7]\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )\n",
    "net.HyparamGen()\n",
    "\n",
    "PerpetualSearch(net, X, Y, aktuellMAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check stability with respect to alternate train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 1729\n",
    "while p < 1745:\n",
    "    p += 1\n",
    "    np.random.seed(p)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    folds = 3\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "    i = np.argmin(record[\"Val MAPE\"])\n",
    "    net = Network(\n",
    "                  [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "                  [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "                  record.iloc[i, 4],                # learning rate\n",
    "                  record.iloc[i, 5],                # learning rate decay\n",
    "                  2048,                             # desired batch size\n",
    "                  int(record.iloc[i, 6]),           # number of epochs\n",
    "                  MAPE,                             # loss function\n",
    "                  X.shape[1]                        # max number of features\n",
    "                 )\n",
    "    print \"doing something\"\n",
    "    # focus on top N features\n",
    "    correlindices = top_features(X, Y)\n",
    "    tops = correlindices[0:net.sizes[0]]\n",
    "    X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "    currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                           X_test_tops, Y_test, CVfolds)\n",
    "    with open(\"Stability.csv\",\"a+\") as f:\n",
    "        for sz in net.sizes[:-1]:\n",
    "            f.write(\"%s\\t\" % sz)\n",
    "        f.write(\"%s\\t\" % net.eta)\n",
    "        f.write(\"%s\\t\" % net.alpha)\n",
    "        f.write(\"%s\\t\" %net.epochs)\n",
    "        f.write(\"%s\\t\" % currentMAPE)\n",
    "        f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data for some wrap-up analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1729) # ensures test labels are identical to those in grid search\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "index_train = X_train[:,0:2]\n",
    "index_test = X_test[:,0:2]\n",
    "X_train = X_train[:,3:]\n",
    "X_test = X_test[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess-1 MAPE results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the guess-1 result for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26865339812767497"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess1 = np.ones_like(Y_test)\n",
    "MAPE(guess1, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the guess- result for the validation (training) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26715048000148989"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess1 = np.ones_like(Y_train)\n",
    "MAPE(guess1, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate model performance by districtID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlindices = top_features(X, Y)\n",
    "tops = correlindices[0:net.sizes[0]]\n",
    "X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "\n",
    "net.train(X_train_tops, Y_train)\n",
    "predictions = net.predict(X_test_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7.17064846416 0.462348540742\n",
      "2 0.654700854701 0.0800380757765\n",
      "3 0.666666666667 0.100522480005\n",
      "4 7.406885759 0.449084036595\n",
      "5 0.714516129032 0.104428978923\n",
      "6 4.67288135593 0.290702113859\n",
      "7 36.0771812081 0.645192056597\n",
      "8 50.7475728155 0.557774788163\n",
      "9 2.64827586207 0.334335971237\n",
      "10 1.09726962457 0.154669360846\n",
      "11 1.22770700637 0.167078848796\n",
      "12 5.08731466227 0.49223369266\n",
      "13 0.853503184713 0.114939639722\n",
      "14 15.5024 0.527644054848\n",
      "15 0.20487804878 0.0313661340868\n",
      "16 1.45273631841 0.161720891213\n",
      "17 0.397976391231 0.0518509713958\n",
      "18 1.02950819672 0.149280551802\n",
      "19 2.85441941075 0.311881702831\n",
      "20 11.9548611111 0.503543670642\n",
      "21 3.28109028961 0.356974148462\n",
      "22 9.8352553542 0.479180781292\n",
      "23 49.2926829268 0.583283687693\n",
      "24 18.8778501629 0.502705269177\n",
      "25 2.41138211382 0.215002260073\n",
      "26 9.01196581197 0.411083192847\n",
      "27 7.10837438424 0.503616849793\n",
      "28 18.6722408027 0.518960778034\n",
      "29 2.7559429477 0.31395042464\n",
      "30 0.398692810458 0.0514277171797\n",
      "31 1.68454258675 0.263407310917\n",
      "32 0.623901581722 0.090262530847\n",
      "33 0.698517298188 0.0966668011538\n",
      "34 1.35953177258 0.187746205062\n",
      "35 1.28943089431 0.158806020648\n",
      "36 1.03745928339 0.126628295704\n",
      "37 32.6232114467 0.555836887821\n",
      "38 2.61912751678 0.225399036576\n",
      "39 1.91653027823 0.20044029466\n",
      "40 1.18166383701 0.175388109717\n",
      "41 1.02571860817 0.134514421693\n",
      "42 5.0253968254 0.454461885201\n",
      "43 0.573402417962 0.0817329906524\n",
      "44 0.5 0.0782309406469\n",
      "45 0.645211930926 0.0784832317118\n",
      "46 26.6650326797 0.520199842979\n",
      "47 1.65878378378 0.219515828351\n",
      "48 29.198915009 0.622404218259\n",
      "49 0.635024549918 0.0915615506386\n",
      "50 0.434856175973 0.0614563909082\n",
      "51 106.724957555 0.554313596963\n",
      "52 0.285945072698 0.0372697528948\n",
      "53 1.53921568627 0.15511312914\n",
      "54 0.529051987768 0.0613832870911\n",
      "55 0.235880398671 0.0255407150755\n",
      "56 0.651376146789 0.0885521369743\n",
      "57 0.374183006536 0.0426863599078\n",
      "58 0.744147157191 0.102801323411\n",
      "59 0.509532062392 0.0577129917978\n",
      "60 0.375209380235 0.0517879653926\n",
      "61 0.391822827939 0.0545577424442\n",
      "62 0.240802675585 0.0338013363774\n",
      "63 0.23240589198 0.0253520766705\n",
      "64 0.483306836248 0.0553150383839\n",
      "65 0.322795341098 0.0442830839798\n",
      "66 0.327666151468 0.0391131764734\n"
     ]
    }
   ],
   "source": [
    "combi= np.hstack([index_test, predictions[-1], Y_test])\n",
    "\n",
    "districts = [x for x in xrange(1,67)]\n",
    "districtMAPE, averagegap = [], []\n",
    "for dID in districts:\n",
    "    print dID, \\\n",
    "          np.mean(combi[np.where(combi[:,0]==dID)[0]][:,3]), \\\n",
    "          MAPE(combi[np.where(combi[:,0]==dID)[0]][:,2], combi[np.where(combi[:,0]==dID)[0]][:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate effects of removing Jan 1st data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "import copy\n",
    "\n",
    "def PerpetualSearch2(net, X, Y, aktuellMAPE):\n",
    "    folds = 3\n",
    "    correlindices = top_features(X, Y)\n",
    "    np.random.seed(1729)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    noJ1 = np.where(X_train[:,1]>144)[0]\n",
    "    Y_train = Y_train[noJ1]\n",
    "    X_train = X_train[noJ1]\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    optimal_net = copy.deepcopy(net)\n",
    "    while True:\n",
    "        print \"doing something\"\n",
    "        # focus on top N features\n",
    "        tops = correlindices[0:net.sizes[0]]\n",
    "        X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "        currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                       X_test_tops, Y_test, CVfolds)\n",
    "        with open(\"RecordII.csv\",\"a+\") as f:\n",
    "            for i in range(6):\n",
    "                if i < len(net.sizes)-1:\n",
    "                    f.write(\"%s\\t\" % net.sizes[i])\n",
    "            f.write(\"%s\\t\" % net.eta)\n",
    "            f.write(\"%s\\t\" % net.alpha)\n",
    "            f.write(\"%s\\t\" %net.epochs)\n",
    "            f.write(\"%s\\t\" % currentMAPE)\n",
    "            f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "            if currentMAPE < aktuellMAPE:\n",
    "                net.train(X_train_tops, Y_train)\n",
    "                true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                f.write(\"%s\\n\" % true_test_loss)  \n",
    "                optimal_net = copy.deepcopy(net)\n",
    "                aktuellMAPE = currentMAPE\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        net = copy.deepcopy(optimal_net)\n",
    "        net.HyparamGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing something\n"
     ]
    }
   ],
   "source": [
    "# initialize network object as starting point for perpetual search\n",
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "aktuellMAPE = record.iloc[i, 7]\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )\n",
    "\n",
    "PerpetualSearch2(net, X, Y, np.infty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
