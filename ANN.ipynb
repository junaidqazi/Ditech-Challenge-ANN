{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neuron functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def exp(z):\n",
    "    return np.exp(z)\n",
    "\n",
    "def I(z): # identity function\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss/Cost functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAPE(pred, tgt):\n",
    "    N = tgt.shape[0]\n",
    "    I = tgt > 0\n",
    "    tgt = tgt[I]\n",
    "    pred = pred[I]\n",
    "    return np.sum(np.divide(np.abs(pred - tgt), tgt)) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**derivative functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d(a, f):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        a: a matrix of activations, a = f(z) for some input matrix z\n",
    "        f: a function such that f(z) = a\n",
    "    What it does:\n",
    "        determines a' = df(z)/dz, the derivative of a with respect to z.\n",
    "        For example, if a = exp(z) then a' = a.\n",
    "    Outputs:\n",
    "        the derivative of a with respect to z\n",
    "    \"\"\"\n",
    "    return (np.ones_like(a) if f == I else\n",
    "           np.sign(a) if f == relu else\n",
    "           a*(1 - a) if f == sigmoid else\n",
    "           a if f == exp else ValueError('bad function type'))\n",
    "\n",
    "def dMAPE(pred, tgt):\n",
    "    N = tgt.shape[0]\n",
    "    return np.divide( np.multiply(tgt, np.sign(pred - tgt)), N*(np.multiply(tgt, tgt) + np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomization functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Gauss(n, s):\n",
    "    return n*(1 + np.random.randn()/s)\n",
    "\n",
    "def asymptotic(n, s):\n",
    "    return 1 - (1 - n)/(1 + np.random.randn()/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, flavors, eta, alpha, batch_size, epochs, loss, max_inputs):\n",
    "        np.random.seed(1729)\n",
    "        self.sizes = sizes # number of nodes in each layer, including input layer\n",
    "        self.sizes[0] = min(self.sizes[0], max_inputs)\n",
    "        self.flavors = flavors # function executed by nodes in given layer, excluding input layer\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.loss = loss\n",
    "        self.max_inputs = max_inputs\n",
    "        self.weights = self.__get_weights__()\n",
    "        self.biases = self.__get_biases__()\n",
    "        \n",
    "    def __get_weights__(self):\n",
    "        return [np.matrix(0.01*np.random.rand(fro, to)) \n",
    "                for (to, fro) in zip(self.sizes[1:], self.sizes[:-1])]\n",
    "\n",
    "    def __get_biases__(self):\n",
    "        return [np.matrix(0.01*np.random.rand(to, 1)).T for to in self.sizes[1:]]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            X: a matrix containing rows of input vectors\n",
    "        What it does:\n",
    "            Ths is the feed forward algorithm, It iterates through the network object, \n",
    "            updating the input vector X at each layer of the network using the weights, \n",
    "            biases and node function\n",
    "        Outputs:\n",
    "            a list of matrices corresponding to the activations at each layer of\n",
    "            the matrix. For example, the first matrix is the input matrix, the last\n",
    "            matrix is the prediction matrix. Intermediate matrices are the\n",
    "            activations output by the hidden layers\n",
    "        \"\"\"\n",
    "        A = [X]\n",
    "        for i in xrange(len(self.flavors)):\n",
    "            W, B, F = self.weights[i], self.biases[i], self.flavors[i]\n",
    "            X = F(X*W + B)\n",
    "            A.append(X)\n",
    "        return A\n",
    "            \n",
    "    def __backprop__(self, A, Y):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            A: a list containing matrices of activations for each layer of the\n",
    "            network for the current minibatch\n",
    "            Y: a list of targets for the curent minibatch\n",
    "        What it does:\n",
    "            given a batch of data, finds the gradients of the loss function\n",
    "            with respect to each of the weight and bias matrices\n",
    "        Outputs:\n",
    "            a list containing all partials with respect to weight matrices\n",
    "            and a list containing all partials with respect to bias matrices\n",
    "        \"\"\"\n",
    "        nabla_CB, nabla_CW = [], []\n",
    "        delta = dMAPE(A[-1], Y)\n",
    "        seno = np.ones_like(Y).T # row ones vector\n",
    "        for i in xrange(1, len(A)):\n",
    "            # calculate z-derivative of current activation\n",
    "            da = d(A[-i], self.flavors[-i])\n",
    "            delta_da = np.multiply(delta, da)\n",
    "            nabla_CB.insert(0, seno*delta_da)\n",
    "            nabla_CW.insert(0, A[-i-1].T*(delta_da))\n",
    "            delta = (delta_da)*self.weights[-i].T\n",
    "        return nabla_CB, nabla_CW\n",
    "    \n",
    "    def train(self, X_fit, Y_fit):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            X_fit: the portion of the training feature matrix used for fitting\n",
    "            Y_fit:  the portion of the training target matrix used for fitting\n",
    "            X_val: the portion of the training feature matrix used for validation\n",
    "        What it does:\n",
    "            Iterates over epochs in which the entire fit data matrix is used to\n",
    "            train the network. Controls the batch generation and selection within\n",
    "            epochs. Feeds batches one at a time into the feedforward and\n",
    "            backpropagation functions. Updates the network object's weights and\n",
    "            biases with the negative gradients derived by back prop and scaled by\n",
    "            the learning rate factor. Decays the learning rate each epoch.\n",
    "        Outputs:\n",
    "            an array of predictions for the validation data from the fully trained\n",
    "            network\n",
    "        \"\"\"\n",
    "        np.random.seed(1729)\n",
    "        eta = self.eta # make a copy as we're gonna decay it over time\n",
    "        for ep in xrange(self.epochs):\n",
    "            n = X_fit.shape[0]\n",
    "            indices = np.random.shuffle([i for i in xrange(n)])\n",
    "            batches = n / self.batch_size + 1 # cld --> ceiling division\n",
    "            batch_size = 1.*n / batches # the used batch size is not precisely what\n",
    "                                     # was requested, but optimal given the\n",
    "                                     # amount of data\n",
    "            k = 0\n",
    "            while k < batches:\n",
    "                lo = int(k*batch_size)\n",
    "                hi = int((k+1)*batch_size)\n",
    "                A_list = self.predict(X_fit[lo:hi,:])\n",
    "                nabla_CB, nabla_CW = self.__backprop__(A_list, Y_fit[lo:hi])\n",
    "                self.weights = ([w - eta*dCw for (w, dCw) in zip(self.weights, nabla_CW)])\n",
    "                self.biases = ([b - eta*dCb for (b, dCb) in zip(self.biases, nabla_CB)])\n",
    "                k += 1\n",
    "            eta *= self.alpha\n",
    "\n",
    "    def HyparamGen(self):\n",
    "        np.random.seed()\n",
    "        self.sizes = [int(Gauss(sz, 10)) for sz in self.sizes[0:-1]] + [1] # 1 on end\n",
    "        self.sizes[0] = min(self.sizes[0], self.max_inputs)\n",
    "        self.eta = Gauss(self.eta, 10)\n",
    "        self.alpha = asymptotic(self.alpha, 10)\n",
    "        self.epochs = int(Gauss(self.epochs, 10))\n",
    "        np.random.seed(1729)\n",
    "        self.weights = self.__get_weights__()\n",
    "        self.biases = self.__get_biases__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_features(X, Y):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        X: the full feature matrix\n",
    "        Y: the full target matrix\n",
    "    What it does:\n",
    "        Finds the top features (columns) in the training feature matrix according to \n",
    "        correlation to the training target matrix.\n",
    "    Outputs:\n",
    "        the indices of the feature columns in order of correlation importance\n",
    "    \"\"\"\n",
    "    lyste = abs(X.T * Y)\n",
    "    indices = []\n",
    "    for _ in xrange(len(lyste)):\n",
    "        i = np.argmax(lyste)\n",
    "        lyste[i] = 0.\n",
    "        indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CrossValidation(net, X_train, Y_train, X_test, Y_test, CV):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "        net: a neural network object\n",
    "        X_train: the training feature matrix\n",
    "        Y_train: the training target matrix\n",
    "        X_test: the test feature matrix\n",
    "        Y_test: the test target matrix\n",
    "        CV: an array of arrays containg the fit and train indices for each\n",
    "        cross-validation fold\n",
    "    What it does:\n",
    "        Iterates over cross-validation folds, splitting the data into\n",
    "        appropriate fit and validation sets, and passing the fit and val sets\n",
    "        on to the train function. Assembles validation predictions from the\n",
    "        train function into a comprehensive list over the entire training data\n",
    "        set. Assembles test data predictions from the feedforward function for\n",
    "        every fold.\n",
    "    Outputs:\n",
    "        The validation loss, using whatever loss functon is defined for the\n",
    "        network\n",
    "        The mean of the test data predictions from each fold of the cross\n",
    "        validation\n",
    "    \"\"\"\n",
    "    psi = np.matrix(np.empty((0, 1)))\n",
    "    Y = np.matrix(np.empty((0, 1)))\n",
    "    psi_test = np.matrix(np.empty((X_test.shape[0], 0)))\n",
    "    for in_fit, in_val in CV:\n",
    "        X_fit, Y_fit = X_train[in_fit,:], Y_train[in_fit]\n",
    "        X_val, Y_val = X_train[in_val,:], Y_train[in_val]\n",
    "        #training\n",
    "        net.train(X_fit, Y_fit)\n",
    "        psi = np.vstack((psi, net.predict(X_val)[-1]))\n",
    "        Y = np.vstack((Y, Y_val))\n",
    "        psi_test = np.hstack((psi_test, net.predict(X_test)[-1]))\n",
    "    return (net.loss(psi, Y), net.loss(np.mean(psi_test, axis=1), Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "import copy\n",
    "\n",
    "def PerpetualSearch(net, X, Y, aktuellMAPE):\n",
    "    folds = 3\n",
    "    correlindices = top_features(X, Y)\n",
    "    np.random.seed(1729)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    optimal_net = copy.deepcopy(net)\n",
    "    while True:\n",
    "        print \"doing something\"\n",
    "        # focus on top N features\n",
    "        tops = correlindices[0:net.sizes[0]]\n",
    "        X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "        currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                       X_test_tops, Y_test, CVfolds)\n",
    "        with open(\"Record.csv\",\"a+\") as f:\n",
    "            for i in range(6):\n",
    "                if i < len(net.sizes)-1:\n",
    "                    f.write(\"%s\\t\" % net.sizes[i])\n",
    "            f.write(\"%s\\t\" % net.eta)\n",
    "            f.write(\"%s\\t\" % net.alpha)\n",
    "            f.write(\"%s\\t\" %net.epochs)\n",
    "            f.write(\"%s\\t\" % currentMAPE)\n",
    "            f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "            if currentMAPE < aktuellMAPE:\n",
    "                net.train(X_train_tops, Y_train)\n",
    "                true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                f.write(\"%s\\n\" % true_test_loss)  \n",
    "                optimal_net = copy.deepcopy(net)\n",
    "                aktuellMAPE = currentMAPE\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        net = copy.deepcopy(optimal_net)\n",
    "        net.HyparamGen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Grid Search. \n",
    "\n",
    "Do this first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GridSearch(X,Y, storageloc):\n",
    "    folds = 3\n",
    "    correlindices = top_features(X, Y)\n",
    "    np.random.seed(1729)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    aktuellMAPE = np.infty\n",
    "\n",
    "    for w in [[10,10,10,10], [20,20,20,20], [50,50,50,50], [100,100,100,100]]:\n",
    "        for eta in [0.1, 0.5, 1]:\n",
    "            for alpha in [0.999, 0.99, 0.9]:\n",
    "                for eps in [100, 200, 300, 400]:\n",
    "                    net = Network(\n",
    "                                  w + [1],          # nodes per layer\n",
    "                                  [relu, relu, relu, relu],     # activation functions at each hidden and output layer\n",
    "                                  eta,                                     # learning rate\n",
    "                                  alpha,                                    # learning rate decay\n",
    "                                  2048,                                    # desired batch size\n",
    "                                  eps,                                     # number of epochs\n",
    "                                  MAPE,                                    # loss function\n",
    "                                  X.shape[1]                               # max number of features\n",
    "                                 )\n",
    "                    print \"doing something\"\n",
    "                    # focus on top N features\n",
    "                    tops = correlindices[0:net.sizes[0]]\n",
    "                    X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "                    currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                           X_test_tops, Y_test, CVfolds)\n",
    "                    with open(storageloc,\"a+\") as f:\n",
    "                        for sz in net.sizes[:-1]:\n",
    "                            f.write(\"%s\\t\" % sz)\n",
    "                        f.write(\"%s\\t\" % net.eta)\n",
    "                        f.write(\"%s\\t\" % net.alpha)\n",
    "                        f.write(\"%s\\t\" %net.epochs)\n",
    "                        f.write(\"%s\\t\" % currentMAPE)\n",
    "                        f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "                        if currentMAPE < aktuellMAPE:\n",
    "                            net.train(X_train_tops, Y_train)\n",
    "                            true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                            f.write(\"%s\\n\" % true_test_loss)  \n",
    "                            optimal_net = copy.deepcopy(net)\n",
    "                            aktuellMAPE = currentMAPE\n",
    "                        else:\n",
    "                            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('TrainingData.csv')\n",
    "\n",
    "Y = np.matrix(training_data.iloc[:,2].values).T\n",
    "X = np.matrix(training_data.iloc[:,3:].values)\n",
    "\n",
    "training_data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GridSearch(X,Y, \"Record.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the best known hyperparameters using perpetual search after classic grid search has completed. \n",
    "\n",
    "Perpetual search will run indefinitely; you have to interrupt the kernel to sop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize network object as starting point for perpetual search\n",
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "aktuellMAPE = record.iloc[i, 7]\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )\n",
    "net.HyparamGen()\n",
    "\n",
    "PerpetualSearch(net, X, Y, aktuellMAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check stability with respect to alternate train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 1729\n",
    "while p < 1745:\n",
    "    p += 1\n",
    "    np.random.seed(p)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    folds = 3\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "    i = np.argmin(record[\"Val MAPE\"])\n",
    "    net = Network(\n",
    "                  [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "                  [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "                  record.iloc[i, 4],                # learning rate\n",
    "                  record.iloc[i, 5],                # learning rate decay\n",
    "                  2048,                             # desired batch size\n",
    "                  int(record.iloc[i, 6]),           # number of epochs\n",
    "                  MAPE,                             # loss function\n",
    "                  X.shape[1]                        # max number of features\n",
    "                 )\n",
    "    print \"doing something\"\n",
    "    # focus on top N features\n",
    "    correlindices = top_features(X, Y)\n",
    "    tops = correlindices[0:net.sizes[0]]\n",
    "    X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "    currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                           X_test_tops, Y_test, CVfolds)\n",
    "    with open(\"Stability.csv\",\"a+\") as f:\n",
    "        for sz in net.sizes[:-1]:\n",
    "            f.write(\"%s\\t\" % sz)\n",
    "        f.write(\"%s\\t\" % net.eta)\n",
    "        f.write(\"%s\\t\" % net.alpha)\n",
    "        f.write(\"%s\\t\" %net.epochs)\n",
    "        f.write(\"%s\\t\" % currentMAPE)\n",
    "        f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-read in data for some more detailed analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-initialize X and Y for the next few analyses. They require the index values districtID and UTC\n",
    "X = Y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('TrainingData.csv')\n",
    "\n",
    "Y = np.matrix(training_data.iloc[:,2].values).T\n",
    "X = np.matrix(training_data.iloc[:,:].values)\n",
    "\n",
    "training_data = 0\n",
    "\n",
    "np.random.seed(1729) # ensures test labels are identical to those in grid search\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "\n",
    "index_train = X_train[:,:2]\n",
    "index_test = X_test[:,:2]\n",
    "X_train = X_train[:,3:]\n",
    "X_test = X_test[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate model performance by districtID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlindices = top_features(X, Y)\n",
    "tops = correlindices[:net.sizes[0]]\n",
    "X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "\n",
    "net.train(X_train_tops, Y_train)\n",
    "predictions = net.predict(X_test_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combi= np.hstack([index_test, predictions[-1], Y_test])\n",
    "\n",
    "districts = [x for x in xrange(1,67)]\n",
    "districtMAPE, averagegap = [], []\n",
    "for dID in districts:\n",
    "    print dID, \\\n",
    "          np.mean(combi[np.where(combi[:,0]==dID)[0]][:,3]), \\\n",
    "          MAPE(combi[np.where(combi[:,0]==dID)[0]][:,2], combi[np.where(combi[:,0]==dID)[0]][:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate effects of removing Jan 1st data from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "import copy\n",
    "\n",
    "def PerpetualSearchNoJ1(net, X, Y, aktuellMAPE):\n",
    "    folds = 3\n",
    "    correlindices = top_features(X, Y)\n",
    "    np.random.seed(1729)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n",
    "    noJ1 = np.where(X_train[:,1]>144)[0]\n",
    "    Y_train = Y_train[noJ1]\n",
    "    X_train = X_train[noJ1,3:]\n",
    "    X_test = X_test[:,3:]\n",
    "    CVfolds = KFold(Y_train.shape[0], folds, shuffle=True)\n",
    "    optimal_net = copy.deepcopy(net)\n",
    "    while True:\n",
    "        print \"doing something\"\n",
    "        # focus on top N features\n",
    "        tops = correlindices[:net.sizes[0]]\n",
    "        X_train_tops, X_test_tops = X_train[:,tops], X_test[:,tops]\n",
    "        currentMAPE, pseudo_testMAPE = CrossValidation(net, X_train_tops, Y_train, \n",
    "                                                       X_test_tops, Y_test, CVfolds)\n",
    "        with open(\"RecordNoJ1.csv\",\"a+\") as f:\n",
    "            for i in range(6):\n",
    "                if i < len(net.sizes)-1:\n",
    "                    f.write(\"%s\\t\" % net.sizes[i])\n",
    "            f.write(\"%s\\t\" % net.eta)\n",
    "            f.write(\"%s\\t\" % net.alpha)\n",
    "            f.write(\"%s\\t\" %net.epochs)\n",
    "            f.write(\"%s\\t\" % currentMAPE)\n",
    "            f.write(\"%s\\t\" % pseudo_testMAPE)\n",
    "            if currentMAPE < aktuellMAPE:\n",
    "                net.train(X_train_tops, Y_train)\n",
    "                true_test_loss = net.loss(net.predict(X_test_tops)[-1], Y_test)\n",
    "                f.write(\"%s\\n\" % true_test_loss)  \n",
    "                optimal_net = copy.deepcopy(net)\n",
    "                aktuellMAPE = currentMAPE\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        net = copy.deepcopy(optimal_net)\n",
    "        net.HyparamGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize network object as starting point for perpetual search\n",
    "record = pd.read_csv('Record.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )\n",
    "\n",
    "PerpetualSearchNoJ1(net, X, Y, np.infty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess-1 MAPE results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the guess-1 result for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess1 = np.ones_like(Y_test)\n",
    "MAPE(guess1, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the guess- result for the validation (training) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guess1 = np.ones_like(Y_train)\n",
    "MAPE(guess1, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase prior periods to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data5 = pd.read_csv('TrainingData5.csv')\n",
    "\n",
    "Y = np.matrix(training_data5.iloc[:,2].values).T\n",
    "X = np.matrix(training_data5.iloc[:,3:].values)\n",
    "\n",
    "training_data5 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GridSearch(X,Y, \"Record5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize network object as starting point for perpetual search\n",
    "record = pd.read_csv('Record5.csv', delimiter='\\t')\n",
    "i = np.argmin(record[\"Val MAPE\"])\n",
    "aktuellMAPE = record.iloc[i, 7]\n",
    "net = Network(\n",
    "              [int(n) for n in (record.iloc[i, 0:4].values)] + [1], # nodes per layer\n",
    "              [relu, relu, relu, relu],         # activation functions at each hidden and output layer\n",
    "              record.iloc[i, 4],                # learning rate\n",
    "              record.iloc[i, 5],                # learning rate decay\n",
    "              2048,                             # desired batch size\n",
    "              int(record.iloc[i, 6]),           # number of epochs\n",
    "              MAPE,                             # loss function\n",
    "              X.shape[1]                        # max number of features\n",
    "             )\n",
    "\n",
    "PerpetualSearch(net, X, Y, np.infty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
